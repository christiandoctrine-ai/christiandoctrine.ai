# christiandoctrine.ai chatbot architecture:

## Ingest and embed documents
The first step is to ingest your documents and convert them into embeddings. This involves:
- Converting documents to text
- Splitting the text into chunks
- Using an embedding model to convert each chunk into a vector
- Storing the vector embeddings in a vector store
This vector store contains all the embedded chunks from your documents.

## Create the question prompt
The user will ask a question through the chatbot interface. This question, along with the chat history, forms the prompt.
The question prompt is also converted into an embedding using the same embedding model.

## Retrieve relevant embedded document chunks
Using the vector store that contains all the embedded document chunks, we retrieve the chunks that are most similar to the question prompt embedding.
We calculate the cosine similarity between the question prompt vector and each document chunk vector. This identifies the most relevant chunks. Typically we retrieve the top N most similar document chunks, where N is a configurable parameter.

### Text is converted to vectors
The first step is to actually convert the text into vectors using an embedding model. This represents the text as a high-dimensional vector, typically between 100 to 1536 dimensions.

#### Convert to text
Convert your documents, which could be PDFs, Word docs, etc. to plain text. This ensures the text can be processed by the model.

#### Split the text into chunks
You need to split the large text into smaller chunks, typically around 200-2000 characters. This is because the model has a maximum context length limit.
You can split the text into chunks using:

- Regular expressions to match newline characters
- Fixed length chunks of a certain number of characters

#### Embed the text into vectors
You need to convert the text chunks into vector embeddings using:
- OpenAI's embed function if you're using ChatGPT
- A custom embedding model you've trained
This represents the text as high-dimensional vectors for processing.

#### Store the embeddings in a vector store
The vector embeddings of the text chunks need to be stored in an efficient database called a vector store. 
The vector store allows you to efficiently retrieve similar embeddings based on cosine similarity.


### Similar text has similar vectors
The key property of embeddings is that similar text will have similar vectors. This is because the embedding model has learned this representation from the training data.

### Cosine similarity is calculated
To determine how similar two pieces of text are, we calculate the cosine similarity of their vector embeddings.

The cosine similarity is the cosine of the angle between the two vectors. It ranges from 0 to 1, with 1 being most similar.

### Smaller angle means more similar
Cosine similarity captures that a smaller angle between two vectors means they are more similar.
Visually, similar text has vectors that point in a similar direction, with a smaller angle between them.

### Retrieve similar text
Once you have the vector embeddings stored, you can retrieve similar text by:
- Finding the N most similar embeddings using cosine similarity
- Extracting the original text for those similar embeddings
This allows you to retrieve relevant chunks of text that are similar to a given query.

## Combine relevant chunks with question prompt
The question prompt and relevant document chunk embeddings are combined into a single prompt.
This combined prompt provides the necessary context for ChatGPT to answer the question accurately.

## Send prompt to ChatGPT and get response
The combined prompt of the question and relevant document chunks is sent to ChatGPT.
ChatGPT then generates a response based on the provided context in the prompt.

## Return response to user
The response generated by ChatGPT is returned to the user through the chatbot interface.

## Update chat history
The question, response, and any relevant information is added to the chat history.

This ensures ChatGPT has the necessary context for any follow up questions.

## Rinse and repeat for each question
The entire process is repeated for each question the user asks. This allows for a full conversation with your data.

# Diagram

![|1000](https://cln.sh/Cw2LpTMKFBv7MLtn9CRn/download)
